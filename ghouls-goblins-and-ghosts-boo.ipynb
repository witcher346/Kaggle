{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kojimba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\kojimba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\kojimba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\kojimba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\kojimba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\kojimba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Kojimba\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kojimba\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kojimba\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kojimba\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kojimba\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kojimba\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import sklearn.model_selection as model_selection\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, train_path, test_path):\n",
    "        self.train_df = pd.read_csv(train_path)\n",
    "        self.test_df = pd.read_csv(test_path)\n",
    "        self.encoders = dict()\n",
    "        self.train_data = None\n",
    "        self.targets = None\n",
    "        self.test_data = None\n",
    "        self.X_train = None\n",
    "        self.Y_train = None\n",
    "        self.X_val = None\n",
    "        self.Y_val = None\n",
    "        self.X_test = None\n",
    "        self.Y_test = None\n",
    "        \n",
    "    def split_data(self):\n",
    "        \n",
    "        (self.X_train, self.X_test, \n",
    "         self.Y_train, self.Y_test) = model_selection.train_test_split(self.train_data, \n",
    "                                                                       self.targets, \n",
    "                                                                       test_size=.2)\n",
    "        \n",
    "        (self.X_train, self.X_val, \n",
    "         self.Y_train, self.Y_val) = model_selection.train_test_split(self.X_train, \n",
    "                                                                      self.Y_train, \n",
    "                                                                      test_size=.2)\n",
    "        \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_df = self.train_df.drop('id', axis=1).copy()\n",
    "        test_df = self.test_df.drop('id', axis=1).copy()\n",
    "        \n",
    "        self.encoders['color'] = OneHotEncoder(sparse=False)\n",
    "        self.encoders['type'] = LabelEncoder()\n",
    "        \n",
    "        self.encoders['color'].fit(train_df['color'].to_numpy().reshape(-1, 1))\n",
    "        self.encoders['type'].fit(train_df['type'].to_numpy().reshape(-1, 1))\n",
    "        \n",
    "        color_hot_train = self.encoders['color'].transform(train_df['color'].to_numpy().reshape(-1, 1))\n",
    "        color_hot_test = self.encoders['color'].transform(test_df['color'].to_numpy().reshape(-1, 1))\n",
    "        targets = self.encoders['type'].transform(train_df['type'].to_numpy().reshape(-1, 1))\n",
    "        \n",
    "        train_data = np.zeros((train_df.shape[0], train_df.shape[1]+4))\n",
    "        for i in range(train_data.shape[0]):\n",
    "            train_data[i] = np.concatenate((train_df.to_numpy()[i][:-2], color_hot_train[i]))\n",
    "        \n",
    "        test_data = np.zeros((test_df.shape[0], test_df.shape[1]+5))\n",
    "        for i in range(test_data.shape[0]):\n",
    "            test_data[i] = np.concatenate((test_df.to_numpy()[i][:-1], color_hot_test[i]))\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.targets = targets\n",
    "        self.test_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kojimba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\kojimba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('train.csv', 'test.csv')\n",
    "dataset.prepare_data()\n",
    "dataset.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, model, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.train_predictions = None\n",
    "        self.val_predictions = None\n",
    "        self.test_predictions = None\n",
    "    \n",
    "    def fit(self):\n",
    "        self.model.fit(self.dataset.X_train, self.dataset.Y_train)\n",
    "    \n",
    "    def test(self):\n",
    "        val_predictions = self.model.predict(self.dataset.X_val)\n",
    "        train_predictions = self.model.predict(self.dataset.X_train)\n",
    "        self.val_predictions = val_predictions\n",
    "        self.train_predictions = train_predictions\n",
    "        \n",
    "    def predict(self):\n",
    "        self.test_predictions = self.model.predict(self.dataset.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from keras import layers, models, optimizers\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, activation, units, lr, optimizer, init, beta1=0.9, beta2=0.999):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(activation=activation, units=units, init=init, input_shape=(input_shape, ), kernel_regularizer='l2'))\n",
    "    model.add(layers.Dense(activation=activation, units=units, init=init, kernel_regularizer='l2'))\n",
    "    model.add(layers.Dense(activation='softmax', units=3, kernel_regularizer='l2'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer(lr=lr, beta_1=beta1, beta_2=beta2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [.0001,]\n",
    "inits = ['glorot_uniform']\n",
    "activations = ['relu']\n",
    "beta1s = [0.9]\n",
    "beta2s = [0.999]\n",
    "units = [50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kojimba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, input_shape=(10,), kernel_regularizer=\"l2\", kernel_initializer=\"glorot_uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model with parameters: (0.0001, 'glorot_uniform', 'relu', 50, <class 'keras.optimizers.Adam'>, 0.9, 0.999, 2)\n",
      "Training took 54.95100021362305 ms\n"
     ]
    }
   ],
   "source": [
    "models_results = {}\n",
    "for lr in learning_rates:\n",
    "    for init in inits:\n",
    "        for activation in activations:\n",
    "            for beta1 in beta1s:\n",
    "                for beta2 in beta2s:\n",
    "                    for unit in units:\n",
    "                        start = time.time()\n",
    "                        model = build_model(dataset.X_train.shape[1], activation, \n",
    "                                            units=unit, lr=lr, optimizer=optimizers.Adam, \n",
    "                                            init=init, beta1=beta1, beta2=beta2)\n",
    "\n",
    "                        model.fit(dataset.X_train, dataset.Y_train, validation_split=0, epochs=3000, verbose=0)\n",
    "\n",
    "                        parameters = (lr, init, activation, unit, optimizers.Adam, beta1, beta2, 2)\n",
    "\n",
    "                        models_results[parameters] = {'training': model.predict(dataset.X_train),\n",
    "                                                      'val': model.predict(dataset.X_val),\n",
    "                                                      'test': model.predict(dataset.X_test)}\n",
    "\n",
    "                        print(f'Created model with parameters: {parameters}')\n",
    "                        print(f'Training took {time.time()-start} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_hot(y_pred):\n",
    "    for row in range(y_pred.shape[0]):\n",
    "        max_index = np.argmax(y_pred[row])\n",
    "        for col in range(y_pred.shape[1]):\n",
    "            y_pred[row, col] = np.floor(y_pred[row, col]) if col != max_index else np.ceil(y_pred[row, col])\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "for param_set in models_results.keys():\n",
    "    model = models_results[param_set]\n",
    "    set_accuracies = {}\n",
    "    for key, y_true in zip(model.keys(), [dataset.Y_train, dataset.Y_val, dataset.Y_test]):\n",
    "        array = model[key].copy()\n",
    "        set_accuracies[key] = accuracy_score(y_true, pred_to_hot(array))\n",
    "    accuracies[param_set] = set_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>training</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0001</th>\n",
       "      <th>glorot_uniform</th>\n",
       "      <th>relu</th>\n",
       "      <th>50</th>\n",
       "      <th>&lt;class 'keras.optimizers.Adam'&gt;</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.999</th>\n",
       "      <th>2</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           test  \\\n",
       "0.0001 glorot_uniform relu 50 <class 'keras.optimizers.Adam'> 0.9 0.999 2  0.76   \n",
       "\n",
       "                                                                           training  \\\n",
       "0.0001 glorot_uniform relu 50 <class 'keras.optimizers.Adam'> 0.9 0.999 2  0.720339   \n",
       "\n",
       "                                                                                val  \n",
       "0.0001 glorot_uniform relu 50 <class 'keras.optimizers.Adam'> 0.9 0.999 2  0.683333  "
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(accuracies).T.sort_values(by=['test'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kojimba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, input_shape=(10,), kernel_regularizer=\"l2\", kernel_initializer=\"glorot_uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model with parameters: (0.0001, 'glorot_uniform', 'relu', 50, <class 'keras.optimizers.Adam'>, 0.9, 0.999, 2)\n"
     ]
    }
   ],
   "source": [
    "models_to_solve = {}\n",
    "for lr in learning_rates:\n",
    "    for init in inits:\n",
    "        for activation in activations:\n",
    "            for beta1 in beta1s:\n",
    "                for beta2 in beta2s:\n",
    "                    for unit in units:\n",
    "                        model = build_model(dataset.X_train.shape[1], activation, \n",
    "                                            units=unit, lr=lr, optimizer=optimizers.Adam, \n",
    "                                            init=init, beta1=beta1, beta2=beta2)\n",
    "                        model.fit(dataset.train_data, dataset.targets, validation_split=0, epochs=3000, verbose=0)\n",
    "                        parameters = (lr, init, activation, unit, optimizers.Adam, beta1, beta2, 2)\n",
    "                        models_to_solve[parameters] = model\n",
    "                        print(f'Trained model with parameters: {parameters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 70, 'min_samples_split': 10, 'max_depth': 30}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'max_depth': [5, 10, 20, 30, 50], \n",
    "              'n_estimators': [5, 15, 30, 50, 70, 100], \n",
    "              'min_samples_split': [5, 10, 25, 40, 55, 70]}\n",
    "\n",
    "forest = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=forest, param_distributions=parameters, scoring='accuracy', \n",
    "                                   cv=5, n_jobs=-1, random_state=42)\n",
    "random_search.fit(dataset.X_train, dataset.Y_train)\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9279661016949152\n",
      "Train accuracy: 0.7166666666666667\n",
      "Train accuracy: 0.7066666666666667\n"
     ]
    }
   ],
   "source": [
    "final_forest = RandomForestClassifier(n_estimators=70, max_depth=30, min_samples_split=10, random_state=42)\n",
    "final_forest.fit(dataset.X_train, dataset.Y_train)\n",
    "\n",
    "print(f'Train accuracy: {accuracy_score(dataset.Y_train, final_forest.predict(dataset.X_train))}')\n",
    "print(f'Train accuracy: {accuracy_score(dataset.Y_val, final_forest.predict(dataset.X_val))}')\n",
    "print(f'Train accuracy: {accuracy_score(dataset.Y_test, final_forest.predict(dataset.X_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bone_length</th>\n",
       "      <th>rotting_flesh</th>\n",
       "      <th>hair_length</th>\n",
       "      <th>has_soul</th>\n",
       "      <th>color</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.471774</td>\n",
       "      <td>0.387937</td>\n",
       "      <td>0.706087</td>\n",
       "      <td>0.698537</td>\n",
       "      <td>black</td>\n",
       "      <td>Ghoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.427332</td>\n",
       "      <td>0.645024</td>\n",
       "      <td>0.565558</td>\n",
       "      <td>0.451462</td>\n",
       "      <td>white</td>\n",
       "      <td>Goblin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.549602</td>\n",
       "      <td>0.491931</td>\n",
       "      <td>0.660387</td>\n",
       "      <td>0.449809</td>\n",
       "      <td>black</td>\n",
       "      <td>Ghoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.682867</td>\n",
       "      <td>0.471409</td>\n",
       "      <td>0.356924</td>\n",
       "      <td>white</td>\n",
       "      <td>Ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.361762</td>\n",
       "      <td>0.583997</td>\n",
       "      <td>0.377256</td>\n",
       "      <td>0.276364</td>\n",
       "      <td>black</td>\n",
       "      <td>Ghost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bone_length  rotting_flesh  hair_length  has_soul  color    type\n",
       "0   3     0.471774       0.387937     0.706087  0.698537  black   Ghoul\n",
       "1   6     0.427332       0.645024     0.565558  0.451462  white  Goblin\n",
       "2   9     0.549602       0.491931     0.660387  0.449809  black   Ghoul\n",
       "3  10     0.638095       0.682867     0.471409  0.356924  white   Ghost\n",
       "4  13     0.361762       0.583997     0.377256  0.276364  black   Ghost"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forest.fit(dataset.train_data, dataset.targets)\n",
    "predictions = final_forest.predict(dataset.test_data)\n",
    "dataset.test_df['type'] = dataset.encoders['type'].inverse_transform(predictions)\n",
    "dataset.test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.test_df[['id', 'type']].set_index('id').to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ghost     193\n",
       "Ghoul     178\n",
       "Goblin    158\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.test_df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ghoul     129\n",
       "Goblin    125\n",
       "Ghost     117\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
